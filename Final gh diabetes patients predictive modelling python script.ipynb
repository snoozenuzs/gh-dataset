{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of the right libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b41954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input file path\n",
    "\n",
    "input_file_path = r\"https://raw.githubusercontent.com/snoozenuzs/gh-dataset/main/nhgh.tsv\"\n",
    "\n",
    "temp_df = pd.read_csv(input_file_path, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame information\n",
    "\n",
    "print(temp_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ac17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre - processing\n",
    "\n",
    "critical_columns = ['seqn', 'sex', 'age', 're', 'income',\n",
    "                   'tx', 'dx','wt','ht', 'bmi',\n",
    "                   'leg', 'arml', 'armc', 'waist', 'tri',\n",
    "                   'sub', 'gh', 'albumin', 'bun','SCr']\n",
    "\n",
    "temp_df_cleaned = temp_df.dropna(subset=critical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding for the 3 categorical features(sex, rec, income)\n",
    "\n",
    "# Encode 'sex' column\n",
    "temp_df_cleaned['sex'] = temp_df_cleaned['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "\n",
    "# Define the order for the ordinal encoding of the income ranges\n",
    "income_order = ['[0,5000)', '[5000,10000)', '[10000,15000)', '[15000,20000)', '[20000,25000)',\n",
    "                '[25000,35000)', '[35000,45000)', '[45000,55000)', '[55000,65000)', '[65000,75000)',\n",
    "                '< 20000', '> 20000', '[75000,100000)', '>= 100000']\n",
    "\n",
    "# Apply ordinal encoding to 'income' column\n",
    "ordinal_encoder = OrdinalEncoder(categories=[income_order])\n",
    "temp_df_cleaned['income'] = ordinal_encoder.fit_transform(temp_df_cleaned[['income']])\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder_re = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to 're' column\n",
    "temp_df_cleaned['re'] = label_encoder_re.fit_transform(temp_df_cleaned['re'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling numerical columns\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = temp_df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "temp_df_cleaned[numerical_cols] = scaler.fit_transform(temp_df_cleaned[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking of final temp_df_cleaned and building a correlation matrix amongst the features to understand the dataset\n",
    "print(temp_df_cleaned.head(5))\n",
    "\n",
    "# Building a correlation matrix using Seaborn to understand the data\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = temp_df_cleaned.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb462031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The gh column is chosen as the feature column for prediction in the models later and hence dropped from the datafram\n",
    "To proceed and define it further as target variable Y\"\"\"\n",
    "\n",
    "#Feature X columns, without gh\n",
    "X = temp_df_cleaned.drop(columns=['gh'])\n",
    "\n",
    "\n",
    "#gh being the target variable(anything that is more than or equal to 6.5 is encoded to 1, and others 0)\n",
    "y = (temp_df_cleaned['gh']>=6.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets, with 20% allocated for testing and 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Display the shapes of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "\n",
    "# Initialize the model\n",
    "log_reg_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "log_reg_accuracy = log_reg_model.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65686cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "decision_tree_accuracy = decision_tree_model.score(X_test, y_test)\n",
    "print(\"Decision Trees Accuracy:\", decision_tree_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010369da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "random_forest_model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "random_forest_accuracy = random_forest_model.score(X_test, y_test)\n",
    "print(\"Random Forests Accuracy:\", random_forest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c246d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the best hyper parameters for the Logistic Regression Model \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid_logistic = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'],  # Penalty (L1 or L2 regularization)\n",
    "    'max_iter': [1000, 10000, 100000]  # Maximum number of iterations\n",
    "}\n",
    "# Initialize GridSearchCV with 20 cross validation \n",
    "log_reg_grid_search = GridSearchCV(LogisticRegression(), param_grid_logistic, cv=20)\n",
    "\n",
    "# Perform Grid Search\n",
    "log_reg_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_log_reg_params = log_reg_grid_search.best_params_\n",
    "print(\"Best Hyperparameters for Logistic Regression:\", best_log_reg_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the best hyperparameters\n",
    "best_log_reg_model = LogisticRegression(**best_log_reg_params)\n",
    "\n",
    "# Train the model\n",
    "best_log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "best_log_reg_accuracy = best_log_reg_model.score(X_test, y_test)\n",
    "print(\"Best Logistic Regression Accuracy after Hyperparameter Tuning:\", round(best_log_reg_accuracy,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding hyperparameters for decision tree tuning \n",
    "param_grid_decision_tree = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "decision_tree_grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid_decision_tree, cv=5)\n",
    "\n",
    "# Perform Grid Search\n",
    "decision_tree_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_decision_tree_params = decision_tree_grid_search.best_params_\n",
    "print(\"Best Hyperparameters for Decision Trees:\", best_decision_tree_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac77108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the revised decision tree model\n",
    "# Initialize the model with the best hyperparameters\n",
    "best_decision_tree_model = DecisionTreeClassifier(**best_decision_tree_params)\n",
    "\n",
    "# Train the model\n",
    "best_decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "best_decision_tree_accuracy = best_decision_tree_model.score(X_test, y_test)\n",
    "print(\"Best Decision Trees Accuracy after Hyperparameter Tuning:\", round(best_decision_tree_accuracy, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reduced search space for RandomizedSearchCV\n",
    "param_dist_random_forest = {\n",
    "    'n_estimators': randint(100, 300),  # Sample values between 100 and 300\n",
    "    'max_depth': [None] + list(range(10, 31)),  # Include None and values between 10 and 30\n",
    "    'min_samples_split': randint(2, 11),  # Sample values between 2 and 10\n",
    "    'min_samples_leaf': randint(1, 5)  # Sample values between 1 and 4\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_forest_random_search = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                                 param_distributions=param_dist_random_forest, \n",
    "                                                 n_iter=50,  # Number of parameter settings that are sampled\n",
    "                                                 cv=5, \n",
    "                                                 n_jobs=-1)  # Use all available cores\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_forest_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_random_forest_params = random_forest_random_search.best_params_\n",
    "print(\"Best Hyperparameters for Random Forests:\", best_random_forest_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hyperparameter random forest model\n",
    "\n",
    "# Initialize the model with the best hyperparameters\n",
    "best_random_forest_model = RandomForestClassifier(**best_random_forest_params)\n",
    "\n",
    "# Train the model\n",
    "best_random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "best_random_forest_accuracy = best_random_forest_model.score(X_test, y_test)\n",
    "print(\"Best Random Forests Accuracy after Hyperparameter Tuning:\", round(best_random_forest_accuracy,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test data\n",
    "test_data = {\n",
    "    'seqn': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'sex': ['male', 'female', 'male', 'female', 'male', 'female', 'male', 'female', 'male', 'female'],\n",
    "    'age': [50, 35, 45, 55, 40, 60, 25, 70, 30, 65],\n",
    "    're': ['Mexican American', 'Non-Hispanic White', 'Non-Hispanic Black', 'Other Hispanic', 'Other Race Including Multi-Racial', 'Mexican American', 'Non-Hispanic White', 'Non-Hispanic Black', 'Other Hispanic', 'Other Race Including Multi-Racial'],\n",
    "    'income': ['[5000,10000)', '[15000,20000)', '[25000,35000)', '[35000,45000)', '[45000,55000)', '[55000,65000)', '[65000,75000)', '[75000,100000)', '< 20000', '> 20000'],\n",
    "    'tx': [1, 0, 1, 1, 0, 1, 0, 1, 0, 1],\n",
    "    'dx': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
    "    'wt': [70, 60, 80, 65, 75, 85, 55, 90, 50, 95],\n",
    "    'ht': [170, 165, 175, 160, 180, 155, 185, 150, 190, 145],\n",
    "    'bmi': [24, 22, 26, 25, 27, 23, 28, 21, 29, 20],\n",
    "    'leg': [80, 75, 85, 70, 90, 65, 95, 60, 100, 55],\n",
    "    'arml': [30, 28, 32, 27, 33, 26, 34, 25, 35, 24],\n",
    "    'armc': [28, 26, 30, 25, 31, 24, 32, 23, 33, 22],\n",
    "    'waist': [75, 70, 80, 68, 82, 66, 84, 64, 86, 62],\n",
    "    'tri': [15, 12, 18, 10, 20, 8, 22, 6, 24, 4],\n",
    "    'sub': [20, 18, 22, 16, 24, 14, 26, 12, 28, 10],\n",
    "    'gh': [7, 6.5, 7.5, 6, 8, 5.5, 8.5, 5, 9, 4.5],\n",
    "    'albumin': [4.5, 4.0, 5.0, 3.5, 5.5, 3.0, 6.0, 2.5, 6.5, 2.0],\n",
    "    'bun': [10, 8, 12, 7, 13, 6, 14, 5, 15, 4],\n",
    "    'SCr': [0.8, 0.7, 0.9, 0.6, 1.0, 0.5, 1.1, 0.4, 1.2, 0.3]\n",
    "}\n",
    "\n",
    "# Create DataFrame from test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying encoding and scaling to the test data\n",
    "# Encode 'sex' column\n",
    "test_df['sex'] = test_df['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "\n",
    "# Apply ordinal encoding to 'income' column using the same ordinal_encoder from training data\n",
    "test_df['income'] = ordinal_encoder.transform(test_df[['income']])\n",
    "\n",
    "# Apply label encoding to 're' column in test data\n",
    "test_df['re'] = label_encoder_re.transform(test_df['re'])\n",
    "\n",
    "# Standardize numerical columns\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the given target variable 'gh' from the test data\n",
    "test_features = test_df.drop(columns=['gh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test data using Logistic Regression\n",
    "log_reg_predictions = best_log_reg_model.predict(test_features)\n",
    "\n",
    "# Prediction on the test data using Decision Tree\n",
    "dt_predictions = best_decision_tree_model.predict(test_features)\n",
    "\n",
    "# Prediction on the test data using Random Forest\n",
    "rf_predictions = best_random_forest_model.predict(test_features)\n",
    "\n",
    "#Print result of the predictions from three models for sample data\n",
    "print(log_reg_predictions, dt_predictions, rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3022b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d283b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
